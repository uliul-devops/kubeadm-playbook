##################  STORAGE  ################
#############################################

## General Storage settings
## When reseting a previous instalaltion, should it first remove the exsting pvcs&pvs (default false)?
storage:
  delete_pvs: true

##### STORAGE OPTION: VMWARE VSPHERE Storage #
##############################################
##### Note: This requires the cloud provider setting above:
##### ClusterConfiguration.cloudProvider: 'vsphere'

vsphere_storageclass_urls:
  - https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/storage-class/vsphere/default.yaml
  #- https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/vsphere/vsphere-volume-sc-fast.yaml

#vsphere_bug_fix github.com/vmware/kubernetes/issues/495 # For k8s 11.x 
vsphere_bug_fix: False

#####
cloud_config: |
    [Global]
    ## Vsphere:
    ## One must ensure:
    ##  - all vms have this enabled: ./govc vm.change -e="disk.enableUUID=1" -vm=<machine1-x>
    ##  - all vms are in the same VCenter
    ##  - the user below has the following roles at vcenter level:
    ## Datastore > Allocate space
    ## Datastore > Low level file Operations
    ## Virtual Machine > Configuration > Add existing disk
    ## Virtual Machine > Configuration > Add or remove device
    ## Virtual Machine > Configuration > Remove disk
    ## Virtual machine > Configuration > Add new disk
    ## Virtual Machine > Inventory > Create new
    ## Network > Assign network
    ## Resource > Assign virtual machine to resource pool
    ## Profile-driven storage -> Profile-driven storage view
    [Global]
      user = user@corp.example.com
      password = PASSWORD
      server = vcenter.corp.example.com:443
      insecure-flag = 1
      datacenter = DC01
      datastore = DS01
      ##./govc vm.info -vm.dns=machine01 | grep Path #and remove the machine name (last string)
      ## Working dir is necessary when your machines are under a directory (and all have to be under the same one)
      # working-dir = kubernetes

      ## Setup of per machine vm-uuid is usually not required, and it's determined automatically.
      #cat /sys/class/dmi/id/product_serial   and format like: "4237558d-2231-78b9-e07e-e9028e7cf4a5"
      #or: ./govc vm.info -vm.dns=machine01 | grep UUID #(well formated also)
      #machine01: vm-uuid="4215e1de-26df-21ec-c79e-2105fe3f9ad1"
      #machine02: vm-uuid="4215f1e4-6abd-cff1-1a4c-71ec169d7b11"
    [Disk]
      #scsicontrollertype = lsilogic-sas
      scsicontrollertype = pvscsi

#####

##### STORAGE OPTION: Self Created NFS ###
##########################################
## Creates a nfs server on the master and exports the below path from the master to all cluster
nfs_k8s: #https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/nfs/provisioner/nfs-server-gce-pv.yaml
         #https://github.com/kubernetes-incubator/nfs-provisioner
  #enabled: "true"
  enabled: False
  provisioner: nfs.k8s
  # Path on the master node:
  host_path: /storage/nfs
  is_default_class: 'true' # case sensitive! Also: only one class can be default. Note that vpshere thin is also trying to be set as default, choose which one you want as default
  wipe: true # When set to true, every reset the files under host_path will be wiped !!!

##### STORAGE OPTION: Rook (ceph) ########
##########################################
## Rook - Ceph Distributed Software Storage
## As per spec section of: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-cluster.yaml

## NOTE: rook/ceph option is not tested for long time. PRs are welcome!
rook:
  enabled: false
  os_packages:
  - jq
  reset:
    storage_delete: true
  ## OLD Installation type, using url. Now we use the helm chart which wraps it.
  #operator_url:
  #  https://github.com/rook/rook/raw/master/demo/kubernetes/rook-operator.yaml
  client_tools_url:
  - https://github.com/rook/rook/raw/master/demo/kubernetes/rook-client.yaml
  - https://github.com/rook/rook/raw/master/demo/kubernetes/rook-tools.yaml
  sharedfs:
    enabled: false
    fs:
    - { name: "sharedfs", replication: 2 } #ceph osd pool set sharedfs-data size 2 && ceph osd pool set sharedfs-metadata size 2
  allowed_consumer_namespaces:  #E.g.: kubectl get secret rook-admin -n rook -o json | jq '.metadata.namespace = "kube-system"' | kubectl apply -f - # as per: https://github.com/rook/rook/blob/master/Documentation/k8s-filesystem.md
  - "kube-system"
  - "default"
  cluster_spec: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-cluster.yaml and https://github.com/rook/rook/blob/master/Documentation/cluster-tpr.md
    versionTag: master-latest
    dataDirHostPath: /storage/rook
    storage:                # cluster level storage configuration and selection
      useAllNodes: true
      useAllDevices: false
      deviceFilter:
      metadataDevice:
      location:
      storeConfig:
        storeType: filestore
        databaseSizeMB: 1024 # this value can be removed for environments with normal sized disks (100 GB or larger)
        journalSizeMB: 1024  # this value can be removed for environments with normal sized disks (20 GB or larger)
  ## Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
  ## nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
  #    nodes:
  #    - name: "172.17.4.101"
  #     directories:         # specific directores to use for storage can be specified for each node
  #     - path: "/rook/storage-dir"
  #   - name: "172.17.4.201"
  #     devices:             # specific devices to use for storage can be specified for each node
  #     - name: "sdb"
  #     - name: "sdc"
  #     storeConfig:         # configuration can be specified at the node level which overrides the cluster level config
  #       storeType: bluestore
  #   - name: "172.17.4.301"
  #     deviceFilter: "^sd."

## ADVANCED rook options:
  rbd:
    enabled: true
    pool_spec: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-storageclass.yaml and https://github.com/rook/rook/blob/master/Documentation/pool-tpr.md
      replication:
        size: 1
      ## For an erasure-coded pool, comment out the replication size above and uncomment the following settings.
      ## Make sure you have enough OSDs to support the replica size or erasure code chunks.
      #erasureCode:
      #  codingChunks: 2
      #  dataChunks: 2

    storageclass_parameters: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-storageclass.yaml
      pool: replicapool
      ## Specify the Rook cluster from which to create volumes. If not specified, it will use `rook` as the namespace and name of the cluster.
      # clusterName: rook
      # clusterNamespace: rook

  ##ceph_conf: as per https://github.com/rook/rook/blob/master/Documentation/advanced-configuration.md
  #ceph_conf: |
  #  [global]
  #  osd crush update on start = false
  #  osd pool default size = 2

  monitoring: # as per: https://github.com/rook/rook/blob/master/Documentation/k8s-monitoring.md
    enabled: true

#####

