---
## Preparations
## Making sure python exists on all nodes, so Ansible will be able to run:
- hosts: all
  gather_facts: no
  become: yes
  become_method: sudo
  pre_tasks:
  - name: Install python2 for Ansible (usually required on ubuntu, which defaults to python3) # Alternativelly, for Ubuntu machines, define var: ansible_python_interpreter=/usr/bin/python3
    raw: test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) || (yum install -y python2 python-simplejson)
    register: output
    changed_when: output.stdout != ""
    tags: always

  - setup: # aka gather_facts
    tags: always # required for tags, see ansible issue: #14228
    
  - name: test min. vars (group_vars/all) are set (ClusterConfiguration and k8s_network_addons_urls)
    debug: msg='Make sure min. vars are set in group_vars/all/ (e.g. ClusterConfiguration and k8s_network_addons_urls)'
    when: 
    - ClusterConfiguration is not defined
    - JoinConfiguration is not defined
    failed_when: 
    - ClusterConfiguration is not defined
    - JoinConfiguration is not defined
    tags: always # always check if we have vars in place

## proper reset of any previous cluster (if any)
- hosts: primary-master
  become: yes
  become_method: sudo
  tags:
  - reset
  - master
  roles:
  - { role: helm, task: helm_reset, tags: [ 'reset', 'helm_reset' ] }
  - { role: storage, task: remove_pvs, tags: [ 'reset', 'storage_reset', 'pvs_reset' ] }
  - { role: storage, task: nfs_reset, tags: [ 'reset', 'storage_reset', 'nfs_reset' ] }
  - { role: storage, task: rook_reset, tags: [ 'reset', 'storage_reset', 'rook_reset' ] }
  - { role: tools, task: reset_drain, tags: [ 'reset', 'node_reset', 'drain', 'node_drain' ] } #done on master, affecting nodes

## nodes -> reset and install common part (for all nodes)
- hosts: nodes
  become: yes
  become_method: sudo
  tags:
  - node
  roles:
  - { role: tools, task: reset, tags: [ 'reset', 'node_reset' ], when: "inventory_hostname not in groups['masters']" }
  - { role: tools, task: weave_reset, tags: [ 'reset', 'node_reset', 'network_reset', 'weave_reset', 'weave' ], when: "inventory_hostname not in groups['masters']" }
  - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'node_install', 'node' ], when: "inventory_hostname not in groups['masters']" }

## etcd -> reset and install common part (for all etcds, when not in the same group with master)
# starting 1.14 no longer supported
# - hosts: etcd
#   become: yes
#   become_method: sudo
#   tags:
#   - etcd
#   roles:
#   - { role: tools, task: reset, tags: [ 'reset', 'etcd_reset' ], when: "inventory_hostname not in groups['masters']" }
#   - { role: tools, task: weave_reset, tags: [ 'reset', 'etcd_reset', 'network_reset', 'weave_reset', 'weave' ], when: "inventory_hostname not in groups['masters']" }
#   - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'etcd_install', 'node' ], when: "inventory_hostname not in groups['masters']" }
    
## master -> reset and install common part (for all masters - and sometimes etcd when colocated with masters)
- hosts: masters
  become: yes
  become_method: sudo
  tags:
  - master
  roles:
  - { role: tools, task: reset, tags: [ 'reset', 'master_reset' ] }
  - { role: tools, task: weave_reset, tags: [ 'reset', 'master_reset', 'network_reset', 'weave', 'weave_reset' ] }
  - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'master_install'] }

## etcd -> install etcd (when HA)
# starting 1.14 no longer supported
# - hosts: etcd
#   become: yes
#   become_method: sudo
#   any_errors_fatal: yes
#   tags:
#   - etcd
#   - install
#   - etcd_install
#   - ha
#   roles:
#   - { role: etcd, tags: [ 'install', 'etcd_install', 'ha', 'etcd'], when: "groups['masters'] | length > 1" } 

## master -> install keepalived and deploy etcd client certs on masters (relevat if HA)
- hosts: masters
  become: yes
  become_method: sudo
  any_errors_fatal: yes
  tags:
  - master
  - install
  - ha
  - master_install
  roles:
  - { role: keepalived, tags: [ 'master', 'install', 'master_install', 'ha', 'keepalived'], when: "groups['masters'] | length > 1" }
  #1.14 removed: - { role: client-certs, tags: [ 'master', 'install', 'master_install', 'ha', 'client-certs'], when: "groups['masters'] | length > 1" }

- hosts: primary-master 
  name: primary-master (or master in general) - it applies to both ha and non-ha
  become: yes
  become_method: sudo
  any_errors_fatal: yes
  tags:
  - master
  - install
  - master_install
  - ha
  roles:
  - { role: master, tags: [ 'primary-master', 'master', 'install', 'master_install'] } 

- hosts: secondary-masters
  become: yes
  become_method: sudo
  any_errors_fatal: yes
  tags:
  - master
  - install
  - ha
  - master_install
  roles:
  - { role: master, tags: [ 'secondary-masters', 'master', 'install', 'master_install', 'secondary_masters'] } 

## node -> install nodes (kubeadm join, etc)
- hosts: nodes
  become: yes
  become_method: sudo
  any_errors_fatal: yes
  tags:
  - node
  - install
  - node_install
  roles:
  - { role: node, tags: [ 'node', 'install', 'node_install'], when: "inventory_hostname not in groups['masters']" }

## Post deploy (network, storage, helm installation, helm charts deploy, any other addons)
- hosts: primary-master
  become: yes
  become_method: sudo
  tags:
  - post_deploy
  roles:
  - { role: post_deploy, task: all, tags: [ 'post_deploy' ] }
  - { role: storage, task: create_all, tags: [ 'storage', 'rook', 'nfs', 'vsphere' ] }
  - { role: helm, task: all, tags: [ 'helm', 'post_deploy' ] }

### For fixes like vsphere's bug, we have to reboot after some more fixes...
#https://github.com/vmware/kubernetes/issues/495
- hosts: mustrebootlist
  gather_facts: no
  become: yes
  become_method: sudo
  tags:
  - mustrebootlist
  - vsphere_bug_fix
  - vsphere
  roles:
  - { role: tools, task: reboot, tags: [ 'reboot_minimal' ], when: "ClusterConfiguration.cloudProvider is defined and ClusterConfiguration.cloudProvider == 'vsphere' and allow_restart | default(False) and vsphere_bug_fix is defined and vsphere_bug_fix" }

## Generic Sanity
- hosts: masters
  become: yes
  become_method: sudo
  tags:
  - master
  pre_tasks:
  - name: remove temporary mustreboot temporary group
    group:
      name: mustrebootlist
      state: absent
  roles:
  - { role: tools, task: cluster_sanity, tags: [ 'cluster_sanity', 'sanity' ] }
  - { role: tools, task: postinstall_messages, tags: [ 'cluster_sanity', 'sanity' ] }

## to reset/add only some (more) nodes:
##   1. keep in hosts only:
##      - the master
##      - the affected node (all other nodes should not be there)
##   2. Have the token defined in the group_vars/all
##   3. Run using only this/these tag(s):
## ansible-playbook -i hosts -v site.yml --tags "node"   # same with: ansible-playbook -i hosts -v site.yml --tags "node_reset,node_install,cluster_sanity,cluster_info"

## To get cluster info/sanity:
## ansible-playbook -i hosts -v site.yml --tags "cluster_sanity,cluster_info"
