---
- set_fact:
    env_kc: '{{ proxy_env |default({}) | combine ({"KUBECONFIG" :"/etc/kubernetes/admin.conf"}) }}'
  tags:
  - always

- name: decide_master_name for join
  include_role:
    name: common
    tasks_from: decide_master_name

#- name: Install packages required by rook (ceph) storage setup
#  package: name={{ item }} state={{ package_state | default ('latest') }}
#  when: rook is defined and rook.enabled
#  with_items:
#  - jq
#  - ceph-common

## PULL IMAGES
- name: Pull master images on master
  #TODO: to be removed/replaced in 1.11 with the kubeadm pull command (kubeadm config images pull --kubernetes-version v11.0.3 ), but better remove, as kubeadm pre-flight does the pull now anyway...
  #command: /usr/bin/docker pull "{{ item }}:{{ ClusterConfiguration.kubernetesVersion | default ('latest') }}"
  #command: /usr/bin/docker pull "{{ images_repo |default ('k8s.gcr.io') }}/{{ item }}:{{ ClusterConfiguration.kubernetesVersion | default ('latest') }}"
  command: /usr/bin/docker pull "{{ item.name }}:{{ item.tag }}"
  with_items: "{{ DOCKER_IMAGES }}"
  tags:
  - prepull_images
  register: command_result
  changed_when: '"Image is up to date" not in command_result.stdout or "Already exists" not in command_result.stdout'
  #when: pre_pull_k8s_images is defined and pre_pull_k8s_images
  #when: pre_pull_k8s_images is defined and pre_pull_k8s_images and ClusterConfiguration.kubernetesVersion is defined
  #when: full_kube_reinstall is defined and full_kube_reinstall and ClusterConfiguration.kubernetesVersion is defined
  when: 
  - DOCKER_IMAGES is defined
  - pre_pull_k8s_images is defined 
  - pre_pull_k8s_images 
  - images_repo is defined
  #- ClusterConfiguration.kubernetesVersion is defined
  #- ClusterConfiguration.apiVersion == "kubeadm.k8s.io/v1alpha1"
  #when: full_kube_reinstall is defined and full_kube_reinstall and ClusterConfiguration.kubernetesVersion is defined

- name: Pull images on master (from internet when images_repo is not defined)
  command: "kubeadm config images pull --kubernetes-version {{ ClusterConfiguration.kubernetesVersion }} --image-repository images_repo |default ('k8s.gcr.io') "
  tags:
  - prepull_images
  register: command_result
  changed_when: '"Image is up to date" not in command_result.stdout or "Already exists" not in command_result.stdout'
  when: 
  - pre_pull_k8s_images is defined 
  - pre_pull_k8s_images 
  #- DOCKER_IMAGES is not defined ## or images_repo is not defined
  - ClusterConfiguration.kubernetesVersion is version_compare ( 'v1.11', '>=' )

## moved to reset.yml
# - name: Remove /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf if present from HA etcd setup time (in MasterHA)
#   file: "path=/etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf state=absent"

- name: making sure there is an apiServer section under ClusterConfiguration
  set_fact:
    ClusterConfiguration: "{{ ClusterConfiguration | combine ({ 'apiServer' : {} }, recursive=True) }}"
  when: ClusterConfiguration is defined

- name: making sure there is a controllerManager section under ClusterConfiguration
  set_fact:
    ClusterConfiguration: "{{ ClusterConfiguration | combine ({ 'controllerManager' : {} }, recursive=True) }}"
  when: ClusterConfiguration is defined

## SANs
- name: adding {{ inventory_hostname,inventory_hostname_short }} to the ClusterConfiguration.apiServer.certSANs
  set_fact:
    extended_cert_list: "{{ [ansible_default_ipv4.address,inventory_hostname,inventory_hostname_short,custom.networking.masterha_fqdn,'127.0.0.1',custom.networking.masterha_ip | default(ansible_fqdn)] | union (ClusterConfiguration.apiServer.certSANs | default([]) ) }}"
    #extended_cert_list: "{{ [inventory_hostname,inventory_hostname_short] | union (ClusterConfiguration.apiServer.certSANs | default([kubernetes]) ) }}"
  when: ClusterConfiguration is defined 

- name: merging {{ inventory_hostname,inventory_hostname_short }} to the ClusterConfiguration.apiServer.certSANs
  set_fact:
    ClusterConfiguration: "{{ ClusterConfiguration | combine ({ 'apiServer' : {'certSANs' : extended_cert_list } }, recursive=True) }}"
  when: extended_cert_list is defined and ClusterConfiguration is defined

## Cloud-Config - pre cluster init steps
  ## Add the cloud-config to the ClusterConfiguration for kubeadm.k8s.io/v1alpha2 and above
  ## Note: ClusterConfiguration is used only on the primary-master. The secondary-masters take it dynamically.
  ## as the ClusterConfiguration.cloudProvider has been deprecated. We use ClusterConfiguration.cloudProvider as conditional for triggering below
  ## as well as triggering updates to the /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (in some other step)
- block:
  - name: add cloud-config to apiServer.extraVolumes
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'apiServer' : {'extraVolumes': ClusterConfiguration.apiServer.extraVolumes | default ([]) | union ([{'name': 'cloud', 'hostPath': '/etc/kubernetes/cloud-config',  'mountPath': '/etc/kubernetes/cloud-config' }] ) } }, recursive=True ) }}"

  - name: add cloud-config to apiServer.extraArgs
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'apiServer' : {'extraArgs': ClusterConfiguration.apiServer.extraArgs | default ({}) | combine ( {'cloud-provider': ClusterConfiguration.cloudProvider, 'cloud-config': '/etc/kubernetes/cloud-config' }, recursive=True ) } }, recursive=True ) }}"

  - name: add cloud-config to controllerManager.extraVolumes
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'controllerManager' : {'extraVolumes': ClusterConfiguration.controllerManager.extraVolumes | default ([]) | union ([{'name': 'cloud', 'hostPath': '/etc/kubernetes/cloud-config',  'mountPath': '/etc/kubernetes/cloud-config' }] ) } }, recursive=True ) }}"

  - name: add cloud-config to controllerManager.extraArgs
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'controllerManager' : {'extraArgs': ClusterConfiguration.controllerManager.extraArgs | default ({}) | combine ( {'cloud-provider': ClusterConfiguration.cloudProvider, 'cloud-config': '/etc/kubernetes/cloud-config' } , recursive=True ) } }, recursive=True ) }}"

  - name: InitConfiguration - cloudProvider merging {{ ClusterConfiguration.cloudProvider }} to the InitConfiguration.nodeRegistration.kubeletExtraArgs
    set_fact:
      InitConfiguration: "{{  InitConfiguration | combine ( { 'nodeRegistration': {'kubeletExtraArgs': {'cloud-provider': ClusterConfiguration.cloudProvider, 'cloud-config': '/etc/kubernetes/cloud-config' }  } }, recursive=True) }}"
  when:
  - ClusterConfiguration is defined
  - ClusterConfiguration.cloudProvider is defined
  - ( ClusterConfiguration.apiVersion >= "kubeadm.k8s.io/v1alpha3" ) or (ClusterConfiguration.apiVersion == "kubeadm.k8s.io/v1")

### MasterHA: ETCD Setup on master
# section removed starting v1.14

### MasterHA : point api.controlPlaneEndpoint and api.advertiseAddress to {{ custom.networking.masterha_ip }}
- block:
  - name: MasterHA merging {{ custom.networking.masterha_ip }} to the ClusterConfiguration.controlPlaneEndpoint
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( {'controlPlaneEndpoint': custom.networking.masterha_ip + ':' + custom.networking.masterha_bindPort | default (6443) | string }, recursive=True) }}"

  ## TODO: try to remove this, and keep the autodetermined addr:
  # - name: MasterHA merging {{ ansible_default_ipv4.address }} to the InitConfiguration.localAPIEndpoint.advertiseAddress ; kubeadm determines addr auto (and no DNS; ip is mandatory);
  #   set_fact:
  #     InitConfiguration: "{{  InitConfiguration | combine ( { 'localAPIEndpoint': {'advertiseAddress': ansible_default_ipv4.address } }, recursive=True) }}"
  when: 
  - groups['masters'] | length > 1
  - inventory_hostname in groups['primary-master']

### Configuration is prepared, show it, write it, use it
- name: "debug: This is the master init configuration to be used (verbosity 2 or above):"
  debug: var={{item}} verbosity=2
  changed_when: false
  with_items:
  - ClusterConfiguration
  - InitConfiguration
  - KubeProxyConfiguration
  - KubeletConfiguration

- name: Make sure /etc/kubernetes folder exists
  file: path=/etc/kubernetes/ state=directory mode=0755

- name: Writing ClusterConfiguration and InitConfiguration to /etc/kubernetes/kubeadm-master.conf
  copy:
    dest: /etc/kubernetes/kubeadm-master.conf
    force: yes
    content: |
      {{ ClusterConfiguration | to_nice_yaml }}
      ---
      {{ InitConfiguration | to_nice_yaml }}
      ---
      {{ KubeProxyConfiguration | to_nice_yaml }}
      ---
      {{ KubeletConfiguration | to_nice_yaml }}
  when:
  - inventory_hostname in groups['primary-master']

# moved to join.yml
# - name: Writing  JoinConfiguration to /etc/kubernetes/kubeadm-master.conf for secondary-masters
#   copy:
#     dest: /etc/kubernetes/kubeadm-master.conf
#     force: yes
#     content: |
#       {{ JoinConfiguration | to_nice_yaml }}
#       ---
#       {{ KubeProxyConfiguration | to_nice_yaml }}
#       ---
#       {{ KubeletConfiguration | to_nice_yaml }}
#   when:
#   - inventory_hostname in groups['secondary-masters']
      
# - name: Make sure that kubelet is not up (when on secondary-masters)
#   service: name=kubelet state=stopped
#   when:
#   - groups['masters'] | length > 1
#   - inventory_hostname not in groups['primary-master']

# Removed starting v1.14
# - name: Unarchive master-certs.tar.gz to /etc/kubernetes/pki/ (when on secondary-masters)
#   unarchive: copy=yes src=/tmp/master-certs.tar.gz dest=/etc/kubernetes/pki/
#   when:
#   - groups['masters'] | length > 1
#   - inventory_hostname not in groups['primary-master']

# Removed starting v1.14
# - name: kubeadm_init_args when MasterHA and this master node is also an etcd node (need to ignore two preflight errors here)
#   set_fact:
#     kubeadm_init_args: "{{ ' '.join((kubeadm_init_args | default(' '), '--ignore-preflight-errors=FileAvailable--etc-kubernetes-manifests-etcd.yaml', '--ignore-preflight-errors=Port-10250' )) }}"
#     #, '--ignore-preflight-errors=DirAvailable--var-lib-etcd', '--ignore-preflight-errors=Port-2379'
#   when: 
#   - "'etcd' in group_names"
#   - groups['etcd'] | length > 1

- name: "Initialize cluster on primary master with kubeadm init {{kubeadm_init_args}} --config /etc/kubernetes/kubeadm-master.conf --upload-certs"
  # Note: one cannot merge config from both config file anc cli. Only the config file will be used (when present)
#  environment: '{{env_kc}}'
  command: /usr/bin/kubeadm init {{ kubeadm_init_args | default(" ") }} --config /etc/kubernetes/kubeadm-master.conf --upload-certs
  register: kubeadm_init_primary
  tags:
  - init
  when:
  # - groups['masters'] | length > 1 # Allow this for both HA primary and non-HA (exclude ha secondary masters)
  - inventory_hostname in groups['primary-master']

- name: kubeadm_init_primary output
  debug: msg:"{{kubeadm_init_primary.stdout_lines}}"
  when:
  - inventory_hostname in groups['primary-master']

- name: kubeadm_init_primary output var
  debug: var=kubeadm_init_primary verbosity=3
  when:
  - inventory_hostname in groups['primary-master']

- name: "Wait 500 seconds for primary-master to respond: {{ InitConfiguration.localAPIEndpoint.advertiseAddress | default (inventory_hostname) }}:{{ InitConfiguration.localAPIEndpoint.bindPort | default (6443) }} "
  #master_name
  wait_for:
    port: "{{ InitConfiguration.localAPIEndpoint.bindPort | default (6443) }}"
    host: "{{ InitConfiguration.localAPIEndpoint.advertiseAddress | default (ansible_fqdn) }}" #master_name
    delay: 1
    timeout: 500
  run_once: yes
  tags:
  - init
  when:
#  - groups['masters'] | length > 1
  - inventory_hostname in groups['primary-master']

# ### Make sure certs are ready for secondary master and then join cluster
# - block:
#   - set_fact:
#       env_kc: '{{ proxy_env |default({}) | combine ({"PATH" : "/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin"  }) | combine ({"KUBECONFIG" :"/etc/kubernetes/admin.conf"}) }}'

#   - name: "Make sure certs are ready for secondary master to join cluster: kubeadm init phase upload-certs --upload-certs"
#     # Note: one cannot merge config from both config file anc cli. Only the config file will be used (when present)
#     environment: '{{env_kc}}'
#     shell: "/usr/bin/kubeadm init phase upload-certs --upload-certs -- 2>/dev/null | tail -1 "
#     register: kubeadm_upload_certificate_key
#     delegate_to: "{{groups['primary-master'][0]}}"
#     run_once: yes

### TODO: remake it in 1.15 !!!
  # - name: generate a join token on primary-master # TEMPORARY 1.14 till this is fixed: https://github.com/kubernetes/kubeadm/issues/1485
  #   shell: 'kubeadm token create --description "kubeadm-playbook-node-joining-token" --ttl 15m --print-join-command '
  #   # | awk '{print $5}{print $7}'
  #   environment: '{{env_kc}}'
  #   # environment -> is required due to a k8s bug which makes kubeadm need internet to generate a token. setting version is not allowed
  #   # Optionally using "--config /etc/kubernetes/kubeadm-master.conf" to get rid of the message that it tries to connect to internet for version
  #   register: kubeadm_token_whash_secondarymasters
  #   delegate_to: "{{groups['primary-master'][0]}}"
  #   run_once: yes

### TODO: remake it in 1.15 !!!
#### WARNING: 1.14 TEMPORARY REMOVE FROM SECONDARY due to kubeamd issue 1485: --config /etc/kubernetes/kubeadm-master.conf 
  # - #name: "Join cluster with {{kubeadm_init_args}}  --experimental-control-plane --certificate-key {{ kubeadm_upload_certificate_key.stdout_lines[0] }}"
  #   name: '{{ kubeadm_token_whash_secondarymasters.stdout }} {{ kubeadm_init_args | default(" ") }} --experimental-control-plane --certificate-key ...'
  #   command: '{{kubeadm_token_whash_secondarymasters.stdout}} {{ kubeadm_init_args | default(" ") }} --experimental-control-plane --certificate-key {{ kubeadm_upload_certificate_key.stdout_lines[0] }}'
  #   #/usr/bin/kubeadm join
  #   register: kubeadm_join_secondary

  # - name: kubeadm_join_secondary output
  #   debug: msg:"{{kubeadm_join_secondary.stdout_lines}}"

  # - name: kubeadm_join_secondary output var
  #   debug: var=kubeadm_join_secondary verbosity=3

  # when:
  # - groups['masters'] | length > 1 
  # - inventory_hostname not in groups['primary-master']
  # tags:
  # - init
  # - init_secondary_masters

# Removed starting v1.14
# - name: Archive /etc/kubernetes/pki/[cs]a.* (when on primary-master)
#   archive: path=/etc/kubernetes/pki/[cs]a.*,/etc/kubernetes/pki/front-proxy-ca.crt,/etc/kubernetes/pki/front-proxy-ca.key dest=/tmp/master-certs.tar.gz
#   when:
#   - groups['masters'] | length > 1
#   - inventory_hostname in groups['primary-master']

# Removed starting v1.14
# - name: Fetch /tmp/master-certs.tar.gz from primary-master to control machine (when on primary-master)
#   fetch: src=/tmp/master-certs.tar.gz dest=/tmp/ flat=yes
#   when:
#   - groups['masters'] | length > 1
#   - inventory_hostname in groups['primary-master']

# - name: find the token of the newly created cluster
#   command: bash -c '/usr/bin/kubeadm token list | grep -v "<invalid>" | grep "system:bootstrappers" | grep "authentication" | tail -n 1 | cut -f 1 -d" "'
#   register: kubeadm_token
#   when: 
#   - InitConfiguration is not defined or InitConfiguration.bootstrapTokens[0].token is not defined
#     InitConfiguration.token is not defined
#   - inventory_hostname in groups['primary-master'] or inventory_hostname == groups["master"][0]
#   changed_when: false
#   tags:
#   - node  # it is required for the node to register

#- name: populate the token in the ClusterConfiguration
#  set_fact:
#    ClusterConfiguration: "{{ ClusterConfiguration |
#    combine ({ 'token' : ClusterConfiguration_token.stdout }) }}"
#  when: ClusterConfiguration.token is not defined

#This is mandatory especially when proxy is used, and the inventory_hostname is defined with fqdn
#important, in order to ensure the connection to local server is not going via proxy (expecially when applying addons)

- name: update in admin.conf and kubelet.conf the name of primary-master as defined in inventory (e.g. with dns name)
  replace:
    dest: '{{ item }}'
    regexp: '(\s+)(server: https:\/\/)[A-Za-z0-9\-\.]+:'
    replace: '\1\2{{ master_name }}:'
    #backup: yes
  #when: proxy_env is defined ### and master is defined with fqdn in the inventory file (e.g. master.example.com)
  with_items:
  - /etc/kubernetes/admin.conf
  - /etc/kubernetes/kubelet.conf
  #- /etc/kubernetes/controller-manager.conf
  #- /etc/kubernetes/scheduler.conf
  tags:
  - init
  - fqdn
  notify:
  - Restart kubelet

# master node delete and kubelet restart is needed for 1.14+, for cloud=vsphere, otherwise we get: "Unable to find VM by UUID. VM UUID:" or Error "No VM found" node info for node
- name: "when ClusterConfiguration.cloudProvider "
  block:

  - name: vsphere prepare cloud-config-vsphere-secret.yaml
    template:
      src: cloud-config-vsphere-secret.j2
      dest: /etc/kubernetes/cloud-config-vsphere-secret.yaml
      force: yes
    when:
    - inventory_hostname in groups['primary-master']

  - name: "vpshere apply cloud-config-vsphere-secret.yaml "
    environment: '{{env_kc}}'
    command: kubectl apply -f /etc/kubernetes/cloud-config-vsphere-secret.yaml
    when:
    - inventory_hostname in groups['primary-master']

  - name: "vpshere remove cloud-config-vsphere-secret.yaml "
    file:
      path: /etc/kubernetes/cloud-config-vsphere-secret.yaml
      state: absent
    when:
    - inventory_hostname in groups['primary-master']

  ### TODO: remove it in 1.15 !!!
  # - name: temporary hack for 1.14 (which does not allow 2nd master to join by using config file, so no .NodeRegistration.KubeletExtraArgs... )
  #   copy:
  #     dest: /etc/sysconfig/kubelet
  #     content: |
  #       KUBELET_EXTRA_ARGS=--cloud-config=/etc/kubernetes/cloud-config --cloud-provider=vsphere
  #   when:
  #   - inventory_hostname not in groups['primary-master'] # on primary all is ok...
  #   notify:
  #   - Restart kubelet
  
  tags:
  - init
  - vpshere_recreate_master_nodes
  when:
  - ClusterConfiguration.cloudProvider is defined 
  - ClusterConfiguration.cloudProvider == "vsphere"

- name: export KUBECONFIG in master's ~/.bash_profile
  lineinfile: 
    dest: ~/.bash_profile
    line: "export KUBECONFIG=/etc/kubernetes/admin.conf"
    state: present
    create: yes
    regexp: '^export KUBECONFIG=.*'
  when: shell is undefined or shell == 'bash'

- name: Wait few seconds for images pulls and cluster services to start
  pause: seconds=3
  changed_when: false

- name: Forcing restart of services
  meta: flush_handlers

- name: Wait few seconds for images pulls and cluster services to start
  pause: seconds=10
  changed_when: false

- name: when cluster is one machine only, remove NoSchedule taint from master - using inventory_hostname_short
  ## TODO: Use InitConfiguration to remove the taint on master, with the same condition.
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: 'kubectl taint nodes {{ inventory_hostname_short }} {{ item }} --overwrite'
  when:
  - groups['all'] | length == 1
  with_items: #'{{ taints_master }}'
  - 'node-role.kubernetes.io/master:NoSchedule-'
  - 'node-role.kubernetes.io/master=:PreferNoSchedule'
  - 'node-role.kubernetes.io/infra=:PreferNoSchedule'
  tags:
  - taints

- name: when cluster is one machine only, labeling it also as infra node
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: 'kubectl label nodes {{ inventory_hostname_short }} "node-role.kubernetes.io/infra=" --overwrite'
  when:
  - groups['all'] | length == 1
  register: command_result
  changed_when: '"not labeled" not in command_result.stdout'

- name: "sanity - wait for alls pod to be running (besides kube-dns/coredns which won't be ready yet as overlay network is not yet deployed, and workder nodes are not yet installed (on clusters with more than one machine))"
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: "kubectl get --namespace kube-system pods --no-headers | grep -v -w 'Running' | grep -v 'kube-dns' | grep -v 'coredns' || true "
  register: command_result
  tags:
  - sanity
  until: command_result.stdout == ""
  retries: 25
  delay: 5
  changed_when: false

- name: "sanity - make sure master is up (sometimes the above condition is empty as master is in fact not working..."
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: "kubectl get --namespace kube-system pods --no-headers > /dev/null "
  tags:
  - sanity
  delay: 5
  changed_when: false

- name: Set coredns replicas to number of masters (a good practice; by default there are 2 coredns)
  shell: "export KUBECONFIG=/etc/kubernetes/admin.conf; kubectl scale --replicas={{ groups['masters'] | length }} -n kube-system deployment/coredns"
  when:
  - groups['masters'] | length > 1
  - inventory_hostname in groups['primary-master']
  tags:
  - scale
  - scale_dns



