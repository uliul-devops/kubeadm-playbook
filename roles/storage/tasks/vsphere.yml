---
## vsphere/vmware/vcenter
- block:
  - set_fact:
      env_kc: '{{ proxy_env |default({}) | combine ({"KUBECONFIG" :"/etc/kubernetes/admin.conf"}) }}'
    tags:
    - always

  - name: Create vsphere storage class
    command: /usr/bin/kubectl apply -f {{ item }}
    with_items: "{{ vsphere_storageclass_urls | default ([]) }}"
    environment: '{{env_kc}}'
    when:
    - vsphere_storageclass_urls is defined
    tags:
    - vsphere_storageclass_urls
    - vsphere

  - block:
    - name: vsphere_bug_fix github.com/vmware/kubernetes/issues/495
      copy: src=vsphere_bug_fix.sh dest=/tmp/vsphere_bug_fix.sh mode='0755'

    - name: execute vsphere_bug_fix.sh
      environment: '{{env_kc}}'
      shell: /tmp/vsphere_bug_fix.sh
      register: list

    - name: build machine reboot list due to vsphere_bug
      add_host: name={{item}} group=mustrebootlist
      with_items:
        '{{list.stdout_lines}}'

  # NOW it's done via .sh, but in future maybe do:
  # - name: "vpshere bug (No VM found) => so we need to delete master(s) (and restart kubelet)"
  #   environment: '{{env_kc}}'
  #   command: kubectl delete node {{ inventory_hostname_short }}
  #   ignore_errors: true
  #   notify: # kubelet restart is needed for 1.14+, for cloud=vsphere, otherwise we get: "Unable to find VM by UUID. VM UUID:" or Error "No VM found" node info for node
  #   - Restart kubelet

  # # - meta: flush_handlers is not enough, as sometimes delete was not with success, so forcing trigger like this:
  # - name: "vpshere bug (No VM found) =>trigger kubelet restart (after master node deleted)"
  #   debug: msg="vpshere bug =>trigger kubelet restart (after master node deleted)"
  #   notify: 
  #   - Restart kubelet
  #   changed_when: true
    when:
    - vsphere_bug_fix is defined
    - vsphere_bug_fix
    tags:
    - vsphere_bug_fix

  when:
  - ClusterConfiguration.cloudProvider is defined 
  - ClusterConfiguration.cloudProvider == 'vsphere'
  tags:
  - vsphere

