---
#- hosts: all
#  gather_facts: False
#  become: yes
#  become_method: sudo
#  tags:
#  - reset
#  tasks:

- block:

  - name: Reset weave network # if it was used
    shell: /usr/local/bin/weave reset --force
    ignore_errors: yes
  
  - name: remove pods NFS mount leftovers; Note you have to collect them from the remote storage (e.g. vsphere datastore) also
    shell: umount -f $(mount | grep '/kubelet/pods/' | grep '/volumes/kubernetes.io~nfs' | awk '{print $3}')
    tags:
    - umount
    - nfs_reset
    ignore_errors: yes

  - name: Reset cluster (kubeadm reset --force)
    command: /usr/bin/kubeadm reset --force
    ignore_errors: yes
    # TODO: if cluster is installed, but kubedm is no longer available on the machine, we will not have a reset of cluster...

  ### Cleaning full /etc/kubernetes/ ; Starting k8s 1.12 behaves better, at some point we will remove this step:
  - name: ensure old kubeadm config files were removed
    file: state=absent path={{ item }}
    with_items:
    - /etc/kubernetes/
    #- /etc/kubernetes/kubeadm.conf
    #- /etc/kubernetes/kubeadm-master.config
    #- /etc/kubernetes/kubeadm-master.conf
    #- /etc/kubernetes/cloud-config

#  - name: ensure old /etc/kubernetes/ is removed when full_kube_reinstall is true
#    file: state=absent path={{ item }}
#    with_items:
#    - /etc/kubernetes/
#    #- /var/lib/etcd # there might be cases 
#    when: full_kube_reinstall is defined and full_kube_reinstall

  - name: ensure old /var/lib/etcd/member is removed
    file: state=absent path={{ item }}
    with_items:
    - /var/lib/etcd/member
    when: etcd_clean | default(true)

  - name: systemctl stop kube*.*.slice
    shell: 'for i in $(systemctl list-unit-files --no-legend --no-pager -l | grep --color=never -o kube.*\.slice );do echo $i; systemctl stop $i ; done'
    tags:
    - umount

  - name: Reset cluster (kubeadm reset --force) # starting 1.14
    command: /usr/bin/kubeadm reset --force
    ignore_errors: yes
    # TODO: if cluster is installed, but kubedm is no longer available on the machine, we will not have a reset of cluster...

  - name: Remove before reinstall packages
    package: name={{ item }} state=absent
    with_items:
    - kubelet
    - kubeadm
    - kubectl
    - kubernetes-cni
    when: full_kube_reinstall | default (False) #is defined and full_kube_reinstall
    tags:
    - kubelet
    - uninstall

  - name: stop kubelet for cleanup activities
    systemd: name={{ item }} state=stopped
    with_items:
    - kubelet
    - keepalived
    - etcd
    tags:
    - kubelet
    - uninstall
    ignore_errors: yes

  - name: remove plugins mount leftovers; Note you have to collect them from the remote storage (e.g. vsphere datastore) also
    #shell: 'umount $(mount | grep " on /var/lib/kubelet/plugins/kubernetes.io/" | cut -f1 -d" ")'
    shell: umount -f $(mount | grep '/kubelet/plugins/kubernetes.io/' | awk '{print $3}')
    #shell: 'umount $(mount | grep "/kubelet/plugins/kubernetes.io/" | cut -f1 -d" ")'
    tags:
    - kubelet
    - uninstall
    ignore_errors: yes

  - name: remove pods mount leftovers; Note you have to collect them from the remote storage (e.g. vsphere datastore) also
    shell: umount -f $(mount | grep '/kubelet/pods/' | grep '/volumes/kubernetes.io~' | awk '{print $3}')
    tags:
    - kubelet
    - uninstall
    ignore_errors: yes

  #https://github.com/kubernetes/kubernetes/issues/39557
  - name: cni0/cbr0 IP alloction issue
    shell: 'rm -rf /var/lib/cni/ /var/lib/kubelet/* /etc/cni/ ; ip link delete cni0; ip link delete cbr0 ; ip link delete flannel.1; ip link delete weave'
    ignore_errors: yes
    tags:
    - uninstall
    
  - name: ipvsadm clear
    shell: 'ipvsadm --clear'
    ignore_errors: yes
    tags:
    - uninstall    

  - name: Reset iptables rules # THIS TASK SHOULD BE REMOVED, is not maintained
    shell: iptables-save | awk '/^[*]/ { print $1 } /^:[A-Z]+ [^-]/ { print $1 " ACCEPT" ; } /COMMIT/ { print $0; }' | iptables-restore
    when: iptables_reset is defined and iptables_reset
    ignore_errors: yes
    tags:
    - uninstall

  #- name: restart kubelet for cleanup activities
  #  systemd: name={{ item }} state=restarted
  #  with_items:
  #  - kubelet
  #  when: ! (full_kube_reinstall is defined and full_kube_reinstall )
  #  tags:
  #  - kubelet
  #  - uninstall
  #  ignore_errors: yes

  tags:
  - reset
